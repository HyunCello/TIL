# 01 파이토치 패키지의 기본 구성

## 1. torch

메인 네임스페이스

텐서 등의 다양한 수학 함수가 포함되어 있으며 Numpy와 유사한 구조를 가짐

## 2. torch.autograd

자동 미분을 위한 함수들이 포함되어져 있음

자동 미분의 on/off를 제어하는 콘텍스트 매니저(enable_grad/no_grad)나 자체 미분 가능함수를 정의할 때 사용하는 기반 클래스인 'Function'등이 포함되어져 있음

## 3. torch.nn

신경망을 구축하기 위한 다양한 데이터 구조나 레이어 등이 정의되어져 있음

ex. RNN, LSTM과 같은 레이어,ReLU와 같은 활성화 함수, MSELoss와 같은 손실 함수들

## 4. torch.optim

확률적 경사 하강법(Stochastic Gradient Descent, SGD)를 중심으로 한 파라미터 최적화 알고리즘이 구현

## 5. torch.utils.data

SGD의 반복 연산을 실행할 때 사용하는 미니 배치용 유틸리티 함수가 포함되어져 있음

## 6. torch.onnx

ONNX(Open Neural Network Exchange)의 포맷으로 모델을 익스포트(export) 할 때 사용

ONNX는 서로 다른 딥 러닝 프레임워크 간에 모델을 공유할 때 사용하는 포맷

# 02 텐서 조작하기 (Tensor Manipulation) 1

이번 챕터에서 배울 내용 리뷰

벡터, 행렬, 텐서의 개념 이해

Numpy와 파이토치로 벡터, 행렬, 텐서를 다루는 방법에 대해 이해

- 벡터, 행렬 그리고 텐서 (Vector, Matrix and Tensor)

    딥 러닝을 위한 가장 기본적인 수학의 지식인 벡터, 행렬, 텐서에 대해서 알아봄

- 넘파이 훑어보기 (Numpy Review)

    파이토치는 파이썬 패키지 넘파이(Numpy)와 유사. 따라서 넘파이에 대해서 간단히 살펴봄

- 파이토치 텐서 선언하기 (Pytorch Tensor Allocation)

    넘파이로 실습했던 것들을 파이토치로 적용

- 행렬 곱셈 (Matrix Multiplication)

    행렬 연산에 대해 이해하기

- 다른 오퍼레이션들 (Other Basic Ops)

    다른 기본적인 오퍼레이션들에 대해서 이해

## 1. 벡터, 행렬 그리고 텐서

### 1) 벡터, 행렬, 텐서 그림으로 이해하기

<p align="center">
	<img src="./img/tensor1.png" alt="img" width="70%"/>
</p>

딥 러닝을 하게 되면 다루게 되는 가장 기본적인 단위는 벡터, 행렬, 텐서

차원이 없는 값을 스칼라, 1차원으로 구성된 값을 우리는 벡터라고 함

2차원으로 구성된 값을 행렬(Matrix)라고 함

3차원이 되면 우리는 텐서(Tensor)라고 부름

사실 우리는 3차원의 세상에 살고 있으므로, 4차원 이상부터는 머리로 생각하기는 어려움

4차원은 3차원의 텐서를 위로 쌓아올린 모습으로 간주

5차원은 그 4차원을 다시 옆으로 확장한 모습

6차원은 5차원을 뒤로 확장한 모습으로 볼 수 있음

### 2) Pytorch Tensor Shape Convention

사실 딥 러닝을 할 떄 다루고 있는 행렬 또는 텐서의 크기를 고려하는 것은 항상 중요

여기서는 앞으로 행렬과 텐서의 크기를 표현할 때 다음과 같은 방법으로 표기

앞으로 다루게 될 텐서 중 가장 전형적인 2차원 텐서를 예로 들어보자

#### 2D Tensor(Typical Simple Setting)

|t| = (Batch size, dim)

<p align="center">
	<img src="./img/tensor2.png" alt="img" width="70%"/>
</p>

위의 경우 2차원 텐서의 크기 |t|를 (batch size x dimension)으로 표현하였을 경우

조금 쉽게 말하면, 아래의 그림과 같이 행렬에서 행의 크기가 batch size, 열의 크기가 dim이라는 의미

<p align="center">
	<img src="./img/tensor3.png" alt="img" width="70%"/>
</p>

#### 3D Tensor (Typical Computer Vison) - 비전 분야에서의 3차원 텐서

|t| = (batch size, width, height)

<p align="center">
	<img src="./img/tensor4.png" alt="img" width="70%"/>
</p>

일반적으로 자연어 처리보다 비전 분야(이미지, 영상 처리)를 하게 된다면 좀 더 복잡한 텐서를 다루게 된다.

이미지라는 것은 가로, 세로라는것이 존재

그리고 여러 장의 이미지, 그러니까 batch size로 구성하게 되면 아래와 같이 3차원의 텐서가 됨

<p align="center">
	<img src="./img/tensor5.png" alt="img" width="70%"/>
</p>

위의 그림은 세로는 batch size, 가로는 너비(width), 그리고 안쪽으로는 높이(height)가 되는 것을 보여줌

#### 3D Tesnor (Typical Natural Language Processinf) - NLP 분야에서의 3차원 텐서

|t| = (batch size, length, dim)

<p align="center">
	<img src="./img/tensor4.png" alt="img" width="70%"/>
</p>

자연어 처리는 보통 (batch size, 문장 길이, 단어 벡터와 차원)이라는 3차원 텐서를 사용

<p align="center">
	<img src="./img/tensor6.png" alt="img" width="70%"/>
</p>

*NLP 분야의 3D 텐서 예제로 이해하기

4개의 문장으로 구성된 전체 훈련 데이터

```
[[나는 사과를 좋아해], [나는 바나나를 좋아해], [나는 사과를 싫어해], [나는 바나나를 싫어해]]
```

컴퓨터는 아직 이 상태로는 '나는 사과를 좋아해'가 단어가 1개인지 3개인지 이해하지 못함

우선 컴퓨터의 입력으로 사용하기 위해서는 단어별로 나눠줘야 함

```
[['나는', '사과를', '좋아해'], ['나는', '바나나를', '좋아해'], ['나는', '사과를', '싫어해'], ['나는', '바나나를', '싫어해']]
```

이제 훈련 데이터의 크기는 4 X 3의 크기를 가지는 2D 텐서

컴퓨터는 텍스트보다는 숫자를 더 잘 처리할 수 있음

이제 각 단어를 벡터로 만들자

아래와 같이 단어를 3차원의 벡터로 변환했다고 하자

```
'나는' = [0.1, 0.2, 0.9]
'사과를' = [0.3, 0.5, 0.1]
'바나나를' = [0.3, 0.5, 0.2]
'좋아해' = [0.7, 0.6, 0.5]
'싫어해' = [0.5, 0.6, 0.7]
```

위 기준을 따라서 훈련 데이터를 재구성하면...

```
[[[0.1, 0.2, 0.9], [0.3, 0.5, 0.1], [0.7, 0.6, 0.5]],
 [[0.1, 0.2, 0.9], [0.3, 0.5, 0.2], [0.7, 0.6, 0.5]],
 [[0.1, 0.2, 0.9], [0.3, 0.5, 0.1], [0.5, 0.6, 0.7]],
 [[0.1, 0.2, 0.9], [0.3, 0.5, 0.2], [0.5, 0.6, 0.7]]]
```

이제 훈련 데이터는 4 x 3 x 3의 크기를 가지는 3D 텐서

이제 batch size를 2로 하자

```
첫번째 배치 #1
[[[0.1, 0.2, 0.9], [0.3, 0.5, 0.1], [0.7, 0.6, 0.5]],
 [[0.1, 0.2, 0.9], [0.3, 0.5, 0.2], [0.7, 0.6, 0.5]]]

두번째 배치 #2
[[[0.1, 0.2, 0.9], [0.3, 0.5, 0.1], [0.5, 0.6, 0.7]],
 [[0.1, 0.2, 0.9], [0.3, 0.5, 0.2], [0.5, 0.6, 0.7]]]
```

컴퓨터는 배치 단위로 가져가서 연산을 수행

그리고 현재 각 배치의 텐서의 크기는 (2 x 3 x 3)

이는 (batch size, 문장 길이, 단어 벡터의 차원)의 크기

## 2. 넘파이로 텐서 만들기 (벡터와 행렬 만들기)

Pytorch로 텐서를 만들어보기 전에 우선 Numpy로 텐서를 만들어보자

```python
import numpy as np
```

Numpy로 텐서를 만드는 방법은 간단한데 [숫자, 숫자, 숫자]와 같은 방식으로 만들고 이를 np.array()로 감싸주면 됨

### 1) 1D with Numpy

Numpy로 1차원 벡터를 만들어보자

```python
t = np.array([0., 1., 2., 3., 4., 5., 6.])
# 파이썬으로 설명하면 List를 생성해서 np.array로 1차원 array로 변환함
print(t)
```
```
[0. 1. 2. 3. 4. 5. 6.]
```

이제 1차원 벡터의 차원과 크리를 출력해보자

```python
print('Rank of t:', t.ndim)
print('Shape of t: ', t.shape)
```
```py
Rank of t: 1
Shape of t: (7,)
```

.ndim은 몇 차원인지를 출력

1차원은 벡터, 2차원은 행렬, 3차원은 3차원 텐서였음

현제는 벡터이므로 1차원이 출력됨

.shape는 크기를 출력

(7, )는 (1,7)를 의미 -> (1 x 7)의 크기를 가지는 벡터

- 옮긴이 주: 텐서의 크기(shape)를 표현할 때는, (컴마)를 쓰기도 하고 x(곱하기)를 쓰기도 함.

    ex) 2행 3열의 2D 텐서를 표현할 때 (2,3)라고 쓰기도 하고 (2 x 3)이라고 하기도 함. (5,)의 형식은 (1 x 5)를 의미

#### 1-1) Numpy 기초 이해하기

이제 Numpy에서 각 벡터의 원소에 접근하는 방법을 알아보자

Numpy에서 인덱스는 0부터 시작

```py
print('t[0] t[1] t[-1] = ', t[0], t[1], t[-1])
```
```
t[0] t[1] t[-1] = 0.0 1.0 6.0
```

위의 결과는 0번 인덱스를 가진 원소인 0.0, 1번 인덱스를 가진 원소인 1.0, -1번 인덱스를 가진 원소인 6.0이 출력되는 것을 보여줌

-1번 인덱스는 맨 뒤에서부터 시작하는 인덱스

범위 지정으로 원소를 불러올 수도 있음

이를 슬라이싱(Slicing)이라고 함

사용 방법은 [시작 번호 : 끝 번호]를 통해 사용

주의할 점은 슬라이싱은 [시작 번호 : 끝 번호] 라고 했을 때, 끝 번호에 해당하는 것은 포함하지 않음

```py
print('t[2:5] t[4:-1] = ', t[2:5], t[4:-1])
```
```
t[2:5] t[4:-1] = [2. 3. 4.][4. 5.]
```
위의 슬라이싱의 결과를 보자

[2:5] 라고 한다면 2번 인덱스부터 4번 인덱스까지의 결과를 가져온다는 의미

[4:-1]은 4번 인덱스부터 끝에서 첫번째 것까지의 결과를 가져온다는 의미

시작 번호 또는 끝 번호를 생략해서 슬라이싱을 하기도 함

[시작번호 : 끝 번호]에서 시작 번호를 생략하면 처음부터 끝 번호까지 뽑아냄

반면에 [시작 번호: 끝 번호]에서 끝 번호를 생략하면 시작 번호부터 끝까지 뽑아냄

```py
print('t[:2] t[:3])     = ', t[:2], t[3:])
```
```
t[:2] t[3:]      = [0. 1.][3. 4. 5. 6.]
```

### 2) 2D with Numpy

Numpy로 2차원 행렬을 만들어보자

```py
t = np.array([[1., 2., 3.], [4., 5., 6.] [7., 8., 9.], [10., 11., 12.]])
print(t)
```
```
[[ 1.  2.  3.]
 [ 4.  5.  6.]
 [ 7.  8.  9.]
 [10. 11. 12.]]
```
```py
print('Rank of t: ', t.ndim)
print('Shape of t: ', t.shape)
```
```
Rank of t : 2
Shape of t : (4:3)
```
.ndim = 몇 차원인지 출력

현재는 행렬이므로 2차원 출력

.shape는 크기를 출력 -> (4,3) or (4 x 3) -> 행렬이 4행 3열을 의미

Numpy로도 3차원 텐서도 만들 수는 있지만 다음 Pytorch 실습에서 해보자

## 3. 파이토치 텐서 선언하기(PyTorch Tensor Allocation)

파이토치는 Numpy와 매우 유사, 하지만 더 낫다(better)

```py
import torch
```
Numpy를 사용하여 진행했던 실습을 파이토치로 똑같이 해 보자

### 1) 1D with PyTorch

파이토치로 1차원 벡터를 만들어보자

```py
t = torch.FloatTensor([0., 1., 2., 3., 4., 5., 6.])
print(t)
```
dim()을 사용하면 현재 텐서의 차원을 보여줌. shape나 size()를 사용하면 크기를 확인할 수 있음

```py
print(t.dim())  # rank. 즉, 차원
print(t.shape)  # shape
print(t.size()) # shape
```
```
1
torch.Size([7])
torch.Size([7])
```
현재 1차원 텐서, 원소 7개

인덱스 접근과 슬라이싱 해보자

```py
print(t[0], t[1], t[-1])  # 인덱스로 접근
print(t[2:5], t[4:-1])    # 슬라이싱
print(t[:2], t[3:])       # 슬라이싱
```
```
tensor(0.) tensor(1.) tensor(6.)
tensor([2., 3., 4.]) tensor([4., 5.])
tensor([0., 1.]) tensor([3., 4., 5., 6.])
```

### 2) 2D with PyTorch

파이토치로 2차원 행렬을 만들어보자

```py
t = torch.FloatTensor([[1., 2., 3.],
                       [4., 5., 6.],
                       [7., 8., 9.],
                       [10., 11., 12.]
                      ])
print(t)
```
```
tensor([[ 1.,  2.,  3.],
        [ 4.,  5.,  6.],
        [ 7.,  8.,  9.],
        [10., 11., 12.]])
```

dim을 사용하면 현재 텐서의 차원을 보여줌

size()를 사용하면 크기 확인 가능

```py
print(t.dim())  # rank. 즉, 차원
print(t.size()) # shape
```
```
2
torch.Size([4, 3])
```

현재 텐서의 차원은 2차원, (4,3)의 크기를 가짐

슬라이싱을 해 보자

```py
print(t[:, 1]) # 첫
print(t[:, 1].size())
```
```
tensor([2., 5., 8., 11.])
torch.Size([4])
```

위의 결과는 첫번째 차원을 전체 선택하고, 그 상황에서 두번째 차원의 1번 인덱스 값만을 가져온 경우를 보여줌

다시 말해 텐서에서 두번째 열에 있는 모든 값을 가져온 상황

그리고 이렇게 값을 가져온 경우의 크기는 4 (1차원 벡터)

```py
print(t[:, :-1]) # 첫번째 차원을 전체 선택한 상황에서 두 번째 차원에서는 맨 마지막에서 첫번쨰를 제외하고 다 가져온다
```
```
tensor([[ 1.,  2.],
        [ 4.,  5.],
        [ 7.,  8.],
        [10., 11.]])
```
위의 결과는 첫번째 차원을 전체 선택한 상황에서 두번째 차원에서는 맨 마지막에서 첫번째를 제외하고 다 가져오는 경우

### 3) 브로드캐스팅 (Broadcasting)

두 행렬 A, B가 있다고 해보자

행렬의 덧셈과 뺄셈에 대해 알고 있다면.. 이 덧셈과 뺄셈을 할 때는 두 행렬 A, B의 크기가 같아야 한다는 것을 알고 있을 것이다

그리고 두 행렬이 곱셈을 할 때에 A의 마지막 차원과 B의 마지막 차원이 일치해야 함

물론, 이런 규칙들이 있지만 딥 러닝을 하게 되면 불가피하게 크기가 다른 행렬 또는 텐서에 대해 사칙 연산을 수행할 필요가 있는 경우가 생김

이를 위해 파이토치에서는 자동으로 크기를 맞춰서 연산을 수행하게 만드는 브로드캐스팅이라는 기능 제공

우선 같은 크기일 때 연산을 하는 경우

```py
m1 = torch.FloatTensor([[3,3]])
m2 = torch.FloatTensor([[2,2]])
print(m1 + m2)
```
```
tensor([[5.,  5.]])
```

여기서 m1과 m2의 크기는 둘 다 (1,2) -> 문제없이 연산 가능

이번에는 크기가 다른 텐서들 간의 연산

아래는 벡터와 스칼라가 덧셈 연산을 수행하는 것을 보여줌

물론, 수학적으로는 원래 연산이 안되는게 맞지만 파이토치에서는 브로드캐스팅을 통해 이를 연산

```py
# Vector + scalar
m1 = torch.FloatTensor([[1, 2]])
m2 = torch.FloatTensor([3]) #[3] -> [3,3]
print(m1 + m2)
```
```
tensor([[4., 5.]])
```
원래 m1의 크기는 (1,2)이며 m2의 크기는 (1,)이다

그런데 파이토치는 m2의 크기를 (1,2)로 변경하여 연산을 수행

이번에는 벡터 간 연산에서 브로드캐스팅이 적용되는 경우를 보자

```py
# 2 x 1 Vector + 1 x 2 Vector
m1 = torch.FloatTensor([[1, 2]])
m2 = torch.FloatTensor([[3],[4]])
print(m1 + m2)
```
```
tensor([4., 5.],
       [5., 6.])
```
m1의 크기는 (1,2) m2의 크기는 (2,1)

이 두 벡터는 원래 수학적으로는 덧셈 수행 불가..

그러나 파이토치는 두 벡터의 크기를 (2,2)로 변경해서 덧셈을 수행

```py
# 브로드캐스팅 과정에서 실제로 두 텐서가 어떻게 변경되는지 보자.
[1, 2]
==> [[1, 2],
     [1, 2]]
[3]
[4]
==> [[3, 3],
     [4, 4]]
```

브로드캐스팅은 편리하지만, 자동으로 실행되는 기능이므로 사용자 입장에서 굉장히 주의해서 사용해야 함

예를 들어 A 텐서와 B 텐서가 있을 때, 사용자는 이 두 텐서의 크기가 같다고 착각하고 덧셈 연산을 수행했다고 가정해보자

하지만 실제로 이 두 텐서의 크기는 달랐고 브로드캐스팅이 수행되어 덧셈 연산이 수행됨

만약, 두 텐서의 크기가 다르다고 에러를 발생시키면 사용자는 이 연산이 잘못되었을믕 바로 알 수 있지만 브로드캐스팅은 자동으로 수행되므로 사용자는 나중에 원하는 결과가 나오지 않았더라도 어디서 문제가 발생했는지 찾기가 굉장히 어려울 수 있음

### 4) 자주 사용되는 기능들

#### 1) 행렬 곱셈과 곱셈의 차이 (Matrix Multiplication Vs. Multiplication)

행렬로 곱셈을 하는 방법은 크게 두가지가 있다

행렬 곱셈(.matmul)과 원소 별 곱셈(.mul)

파이토치 텐서의 행렬 곱셈을 보자

이는 matmul()을 통해 수행

```
m1 = torch.FloatTensor([[1, 2],[3, 4]])
m2 = torch.FloatTensor([[1], [2]])
print('Shape of Matrix 1: ', m1.shape) # 2 x 2
print('Shape of Matrix 2: ', m2.shape) # 2 x 1
print(m1.matmul(m2)) # 2 x 1
```
```
Shape of Matrix 1 : torch.Size([2, 2])
Shape of Matrix 1 : torch.Size([2, 2])
tensor([[ 5.],
       [11.]])
```

위의 결과는 2 x 2 행렬과 2 x 1 행렬(벡터)의 행렬 곱셈의 결과를 보여줌

행렬 곱셈이 아니라 element-wise 곱셈이라는 것이 존재

이는 동일한 크기의 행렬이 동일한 위치에 있는 원소끼리 곱하는 것을 말함

아래는 서로 다른 크기의 행렬이 브로드캐스팅이 된 후에 element-wise 곱셈이 수행되는 것을 보여줌

이는 * 또는 mul()을 통해 수행함

```
m1 = torch.FloatTensor([[1, 2], [3, 4]])
m2 = torch.FloatTensor([[1], [2]])
print('Shape of Matrix 1: ', m1.shape) # 2 x 2
print('Shape of Matrix 2: ', m2.shape) # 2 x 1
print(m1 * m2) # 2 x 2
print(m1.mul(m2))
```
```
Shape of Matrix 1:  torch.Size([2, 2])
Shape of Matrix 2:  torch.Size([2, 1])
tensor([[1., 2.],
        [6., 8.]])
tensor([[1., 2.],
        [6., 8.]])
```

m1 행렬의 크기는 (2, 2)이였음

m2 행렬의 크기는 (2, 1) 이였음

이 때 element-wise 곱셈을 수행하면, 두 행렬의 크기는 브로드캐스팅이 된 후에 곱셈이 수행됨

더 정확히는 여기서 m2의 크기가 변환됨

```
# 브로드캐스팅 과정에서 m2 텐서가 어떻게 변경되는지 보자
[1]
[2]
==> [[1, 1],
     [2, 2]]
```

#### 2) 평균 (Mean)

평균을 구하는 방법도 제공

이는 Numpy에서의 사용법과 매우 유사함

우선 1차원인 벡터를 선언하여 .mean()을 사용하여 원소의 평균을 구함

```
t = torch.FloatTensor([1, 2])
print(t.mean())
```
```
tensor(1.5000)
```
1과 2의 평균인 1.5가 나옴

이번에는 2차원인 행렬을 선언하여 .mean()을 사용해보자

우선 2차원 행렬을 선언

```
t = torch.FloatTensor([[1, 2], [3, 4]])
print(t)
```
```
tensor([[1., 2.],
        [3., 4.]])
```

2차원 행렬이 선언됨

이제 .mean()을 사용

```
print(t.mean())
```
```
tensor(2.5000)
```

4개의 원소의 평균인 2.5가 나옴

이번에는 dim. 즉, 차원(dimension)을 인자로 주는 경우를 보자

```
print(t.mean(dim=0))
```
```
tensor([2., 3.])
```
dim = 0 이라는 것은 첫번째 차원을 의미

행렬에서 첫번째 차원은 '행'을 의미함

그리고 인자로 dim을 준다 -> 해당 차원을 제거한다는 의미가 됨

다시 말해 행렬에서 '열'만을 남기겠다는 의미

기존 행렬의 크기는 (2, 2)였지만 이를 수행하면 열의 차원만 보존되면서 (1, 2)가 됨

이는 (2,)와 같으며 벡터임

열의 차원을 보존하면서 평균을 구하면 아래와 같이 연산함

```
# 실제 연산 과정
# t.mean(dim = 0)은 입력에서 첫번째 차원을 제거

[[1., 2.],
 [3., 4.]]

1과 3의 평균을 구하고, 2와 4의 평균을 구한다
결과 ==> [2., 3.]
```
