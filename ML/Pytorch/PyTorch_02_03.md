# 03 텐서 조작하기

## 04 뷰(View) - 원소의 수를 유지하면서 텐서의 크기 변경 (매우 중요)

파이토치 텐서의 뷰(View)는 넘파이에서의 리쉐이프(Reshape)와 같은 역할을 함

Reshape라는 이름에서 알 수 있듯이, 텐서의 크기(Shape)를 변경해주는 역할을 함

실습을 위해 우선 임의로 다음과 같이 3차원 텐서를 만듬

```
t = np.array([[[0, 1, 2],
               [3, 4, 5]],
              [[6, 7, 8],
               [9, 10, 11]]])
ft = torch.FloatTensor(t)
```
ft라는 이름의 3차원 텐서를 만듬

크기(Shape)를 확인해보자
```
print(ft.shape)
```
```
torch.Size([2, 2, 3])
```

### 4-1) 3차원 텐서에서 2차원 텐서로 변경

이제 ft 텐서를 view를 사용하여 크기(shape)를 2차원 텐서로 변경해보자
```
print(ft.view([-1, 3]))
print(ft.view([-1, 3]).shape)
```
```
tensor([[ 0.,  1.,  2.],
        [ 3.,  4.,  5.],
        [ 6.,  7.,  8.],
        [ 9., 10., 11.]])
torch.Size([4, 3])
```
view([-1, 3])이 가지는 의미는 이와 같음

-1은 첫번째 차원은 사용자가 잘 모르겠으니 파이토치에 맡기겠다는 의미이고, 3은 두번째 차원의 길이는 3을 가지도록 하라는 의미

다시 말해 현재 3차원 텐서를 2차원 텐서로 변경하되 (?,3)의 크기로 변경하라는 의미

결과적으로 (4,3)의 크기를 가지는 텐서를 얻음

내부적으로 크기 변환은 다음과 같이 이뤄짐

(2, 2, 3) -> (2x2, 3) -> (4, 3)

규칙을 정리해보자

- view는 기본적으로 변경 전과 변경 후의 텐서 안의 원소의 개수가 유지되어야 함
- 파이토치의 view는 사이즈가 -1로 설정되면 다른 차원으로부터 해당 값을 유추함

변경 전 텐서의 원소 수는 (2 x 2 x 3) = 12개였음

그리고 변경 후 텐서의 원소의 개수 또한 (4 x 3) = 12개

### 4-2) 3차원 텐서의 크기 변경

이번에는 3차원 텐서에서 3차원 텐서로 차원은 유지하되, 크기(shape)를 바꾸는 작업을 해보자

view로 텐서의 크기를 변경하더라도 원소의 수는 유지되어야 한다고 언급한 바 있음

그렇다면 (2 x 2 x 3)텐서를 (? x 1 x 3) 텐서로 변경하라고 하면 ?는 몇차원일까 => 4

실습으로 확인해보자

```
print(ft.view([-1, 1, 3]))
print(ft.view([-1, 1, 3]).shape)
```

## 5) 스퀴즈(Squeeze) - 1인 차원을 제거함

스퀴즈는 차원이 1인 경우에는 해당 차원을 제거함

실습을 위해 임의로 (3 x 1)의 크기를 가지는 2차원 텐서를 만든다

```
ft = torch.FloatTensor([[0], [1], [2]])
print(ft)
print(ft.shape)
```
```
tensor([[0.],
        [1.],
        [2.]])
torch.Size([3, 1])
```

해당 텐서는 (3 x 1)의 크기를 가진다

두 번째 차원이 1이므로 squeeze를 사용하면 (3,)의 크기를 가지는 텐서로 변경된다

```
print(ft.squeeze())
print(ft.squeeze().shape)
```
```
tensor([0., 1., 2.])
torch.Size([3])
```
위의 결과는 1이었던 두번째 차원이 제거되면서 (3,)의 크기를 가지는 텐서로 변경되어 1차원 벡터가 된 것을 보여줌

## 6) 언스퀴즈(Unsqueeze) - 특정 위치에 1인 차원을 추가한다

언스퀴즈는 스퀴즈와 정반대

특정 위치에 1인 차원을 추가할 수 있음

실습을 위해 임의로 (3,)의 크기를 가지는 1인 차원 텐서를 만들어보자
```
ft = torch.Tensor([0, 1, 2])
print(ft.shape)
```
```
torch.Size([3])
```
현재는 차원이 1개인 1차원 벡터

여기에 첫번째 차원에 1인 차원을 추가해보자

첫번째 차원의 인덱스를 의미하는 숫자 0을 인자로 넣으면 첫번째 차원이 1인 차원이 추가

```
print(ft.unsqueeze(0))
print(ft.unsqueeze(0).shape)
```
```
tensor([[0., 1., 2.]])
torch.Size([1, 3])
```
위의 결과는 (3,)의 크기를 가졌던 1차원 벡터가 (1,3)의 2차원 텐서로 변경된 것을 보여줌

방금 한 연산을 앞서 배운 view로도 구현 가능함

2차원으로 바꾸고 싶으면서 첫번째 차원을 1이기를 원한다면 view에서 (1, -1)을 인자로 사용하면 됨

```
print(ft.view(1, -1))
print(ft.view(1, -1).shape)
```
위의 결과는 unsqueeze와 view가 동일한 결과를 만든 것을 보여줌

이번에는 unsqueeze의 인자로 1을 넣어보자

인덱스는 0부터 시작하므로 이는 두번째 차원에 1을 추가하겠다는것을 의미

인덱스는 0부터 시작하므로 이는 두번째 차원에 1을 추가하겠다는 것을 의미

현재 크기는 (3,)이었으므로 두번째 차원에 1인 차원을 (3,1)의 크기를 가지게 됨

실습을 진행해보자

```
print(ft.unsqueeze(1))
print(ft.unsqueeze(1).shape)
```

이번에는 unsqueeze의 인자로 -1은 인덱스 상으로 마지막 차원을 의미한다

현재 크기는 (3,)이었으므로 마지막 차원에 1인 차원을 추가하면 (3,1)의 크기를 가지게 됨

다시 말해 현재 텐서의 경우에는 1을 넣은 경우와 -1을 넣은 경우가 동일함

```
print(ft.unsqueeze(-1))
print(ft.unsqueeze(-1).shape)
```
```
tensor([[0.],
        [1.],
        [2.]])
torch.Size([3, 1])
```
맨 뒤에 1인 차원이 추가되면서 1차원 벡터가 (3, 1)의 크기를 가지는 2차원 텐서로 변경됨

## 7) 타입 캐스팅(Type Casting)

텐서에는 자료형이 있음

각 데이터별로 정의되어져 있는데, 예를 들어 32비트의 부동 소수점은 torch.FloatTensor를, 64비트의 부호 있는 정소는 torch.LongTensor를 사용함

GPU 연산을 위한 자료형도 있음

예를 들어 torch.cuda.FloatTensor가 그 예

그리고 이 자료형을 변환하는 것을 타입 캐스팅이라고 함

우선 실습을 위해 long 타입의 lt라는 텐서를 선언

```
lt = torch.LongTensor([1, 2, 3, 4])
print(lt)
```
텐서에다가 .float()를 붙이면 바로 float형으로 타입이 변경됨
```
print(lt.float())
```
```
tensor([1., 2., 3., 4.])
```
이번에는 Byte 타입의 bt라는 텐서를 만들어보자
```
bt = torch.ByteTensor([True, False, False, True])
print(bt)
```
```
tensor([1, 0, 0, 1], dtype=torch.uint8)
```
여기에 .long()이라고 하면 long 타입의 텐서로 변경되고 .float()이라고 하면 float 타입의 텐서로 변경됨

```
tensor([1, 0, 0, 1])
tensor([1., 0., 0., 1.])
```
## 8) 연결하기(concatenate)

이번에는 두 텐서를 연결하는 방법에 대해서 알아보자

우선 (2 x 2)크기의 텐서를 두 개 만듬

```
x = torch.FloatTensor([[1, 2], [3, 4]])
y = torch.FloatTensor([[5, 6], [7, 8]])
```
이제 두 텐서를 torch.cat([])를 통해 연결해보자

그런데 연결 방법은 한 가지만 있는 것이 아니다

torch.cat은 어느 차원을 늘릴 것인지를 인자로 줄 수 있다

예를 들어 dim = 0 은 첫번째 차원을 늘리라는 의미를 담고 있다

```
print(torch.cat([x, y], dim = 0))
```
```
tensor([[1., 2.],
        [3., 4.],
        [5., 6.],
        [7., 8.]])
```
dim = 0 을 인자로 했더니 두 개의 (2 x 2) 텐서가 (4 x 2)텐서가 된 것을 볼 수 있다

dim = 1을 인자로 주자

```
print(torch.cat([x, y], dim = 1))
```
```
tensor([[1., 2., 5., 6.],
        [3., 4., 7., 8.]])
```

dim = 1을 인자로 했더니 두 개의 (2 x 2) 텐서가 (2 x 4) 텐서가 된 것을 볼 수 있다

- 딥 러닝에서는 주로 모델의 입력 또는 중간 연산에서 두 개의 텐서를 연결하는 경우가 많다. 두 텐서를 연결해서 입력으로 사용하는 것은 두 가지의 정보를 모두 사용한다는 의미를 가지고 있음

## 9) 스택킹(Stacking)

연결(concatenate)을 하는 또 다른 방법으로 스택킹(Stacking)이 있다

스택킹은 영어로 쌓는다는 의미

때로는 연결을 하는 것보다 스택킹이 더 편리할 때가 있는데, 이는 스택킹이 많은 연산을 포함하고 있기 때문

실습을 위해 크기가 (2,)로 모두 동일한 3개의 벡터를 만든다

```
x = torch.FloatTensor([1, 4])
y = torch.FloatTensor([2, 5])
z = torch.FloatTensor([3, 6])
```
이제 torch.stack을 통해서 3개의 벡터를 모두 
```
print(torch.stack([x, y, z]))
```
```
tensor([[1., 4.],
        [2., 5.],
        [3., 6.]])
```
스택킹은 사실 많은 연산을 한번에 축약하고 있음

예를 들어 위 작업은 아래의 코드와 동일한 작업
```
print(torch.cat([x.unsqueeze(0), y.unsqueeze(0), z.unsqueeze(0)], dim = 0))
```
x, y, z는 기존에는 전부 (2,)의 크기를 가짐

그런데 .unsqueeze(0)을 하므로서 3개의 벡터는 전부 (1, 2)의 크기의 2차원 텐서로 변경됨

여기에 연결(concatenate)를 의미하는 cat을 사용하면 (3 x 2)텐서가 됨

```
tensor([[1., 4.],
        [2., 5.],
        [3., 6.]])
```
위에서는 torch.stack([x, y, z])라는 한 번의 커맨드로 수행했지만, 연결(concatenate)로 이를 구현하려고 했더니 꽤 복잡해짐

스택킹에 추가적으로 dim을 인자로 줄 수도 있음

이번에는 dim = 1 인자를 주자

이는 두 번째 차원이 증가하도록 쌓으라는 의미로 해석할 수 있음

```
print(torch.stack([x, y, z], dim = 1))
```
```
tensor([[1., 2., 3.],
        [4., 5., 6.]])
```
위의 결과는 두 번째 차원이 증가하도록 스택킹이 된 결과를 보여줌

결과적으로 (2 x 3)텐서가 됨

## 10) one_like와 zeros_like - 0으로 채워진 텐서와 1로 채워진 텐서

실습을 위해 (2 x 3) 텐서를 만든다

```
x = torch.FloatTensor([[0, 1, 2], [2, 1, 0]])
print(x)
```
```
tensor([[0., 1., 2.],
        [2., 1., 0.]])
```

위 텐서에 ones_like를 하면 동일한 크기(shape)지만 1으로만 값이 채워진 텐서를 생성함

```
print(torch.ones_like(x))
```
```
tensor([[1., 1., 1.],
        [1., 1., 1.]])
```

위 텐서에 zeros_like를 하면 동일한 크기(shape)wlaks 0으로만 값이 채워진 텐서를 생성함

```
print(torch.zeros_like(x))
```
```
tensor([[0., 0., 0.],
        [0., 0., 0.]])
```

## 11) In-place Operation (덮어쓰기 연산)
실습을 위해 (2 x 2)텐서를 만들고 x에 저장

```
x = torch.FloatTensor([[1, 2], [3, 4]])
```

곱하기 연산을 한 값과 기존의 값을 출력해보자
```
print(x.mul(2.))
print(x)
```
```
tensor([[2., 4.],
        [6., 8.]])
tensor([[1., 2.],
        [3., 4.]])
```
첫번째 출력은 곱하기 2가 수행된 결과를 보여주고, 두번째 출력은 기존의 값이 그대로 출력된 것을 확인할 수 있음

곱하기 2를 수행했지만 이를 x에다가 다시 저장하지 않았으니, 곱하기 연산을 하더라도 기존의 값 x는 변하지 않는 것이 당연함

그런데 연산 뒤에 _를 붙이면 기존의 값을 덮어쓰기 함 (오오오)

```
print(x.mul_(2.))
print(x)
```
```
tensor([[2., 4.],
        [6., 8.]])
tensor([[2., 4.],
        [6., 8.]])
```
이번에는 x의 값이 덮어쓰기 되어 2 곱하기 연산이 된결과가 출력됨