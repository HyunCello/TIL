차원 축소

# 10.1 차원 축소 개념

## 10.1.1 차원 축소하는 이유

데이터에는 중요한 부분과 중요하지 않은 부분이 있음

여기서 중요하지 않은 부분을 노이즈(noise)라고 함

노이즈는 데이터에서 정보를 얻을 때 방해가 되는 부분

머신러닝 과정에서는 이러한 불필요한 노이즈를 제거하는 것이 중요

그리고 노이즈를 제거할 때 사용하는 방법이 차원 축소(dimension reduction)

차원 축소를 통해 데이터의 중요하지 않은 부분인 노이즈를 제거할 수 있기에 널리 사용되는중

<그림>

[그림 10-1]의 고양이는 3차원 공간에 있음

우리는 고양이의 모습을 보고 고양이라고 인식함

그러나 [그림 10-1]에서 고양이를 직접 보지 않고 고양이라고 인식할 방법이 하나 더 있음

그것은 고양이의 그림자를 보는 경우

즉, 3차원 공간의 고양이가 아니라 2차원 평면의 그림자만 보고도 고양이라고 인식하는 것

이는 고양이를 인식하는 데 3차원 데이터가 아닌 2차원 데이터만으로도 충분하다는 뜻

한편 고양이를 그림자만 보고 판단하려면 햇빛의 각도가 중요

그림자는 햇빛의 각도에 따라 변하는데, 너무 짧거나 길면 고양이라고 인식할 확률이 낮아짐

이처럼 차원 축소는 주어진 데이터의 정보 손실을 최소화하면서 줄이는 것이 핵심

차원 축소는 특징을 추출한다는 것과 밀접한 관련이 있음

특징을 추출한다는 것은 데이터에서 두드러진 면을 찾는 것과 같음

앞서 고양이의 예에서 그림자는 고양이의 중요한 특징을 포함해야 함

즉, 그림자는 고양이라고 판단할 수 있을 법한 정보를 포함해야 한다는 뜻

<br>

차원 축소는 비지도 학습, 지도 학습과 마찬가지로 비지도 학습 차원 축소로 접근할 수도 있고, 지도 학습 차원 축소로 접근할 수도 있음

비지도 학습적인 접근 방법에는 대표적으로 주성분 분석(Principal Component Analysis)과 같은 방법이 있고, 지도 학습적인 접근 방법에는 선형 판별 분석(Linear Discriminant Analysis)과 같은 방법이 있음

## 10.1.3 차원의 저주

차원 축소를 하는 이유 중 하나는 차원의 저주 문제를 해결할 수 있기 때문

**차원의 저주(curse of dimensionality)**란 데이터의 차원이 커질수록 해당 차원을 표현하기 위해 필요한 데이터가 기하급수적으로 많아짐을 의미

예를 들어, 1차원 직선 공간을 생각해 보자

만약 1차원 직선이 2차원 평면만 되어도 단지 1차원 늘어났을 뿐인데, 해당 공간을 나타내기 위해 필요한 데이터는 크게 늘어남

이처럼 차원이 하나씩 늘어날수록 필요한 데이터는 기하급수적으로 많아짐

즉, 트레이닝 데이터 셋의 차원이 클수록 차원의 저주 때문에 해당 공간을 적절히 표현하지 못하여 오버피팅될 확률이 높아짐

다음과 같이 예를 들어보자

<그림>

[그림 10-2]와 같이 1차원 공간에 데이터를 표현할 수 있는 공간이 0부터 10까지 있다고 하면 해당 공간에는 10개의 데이터가 포함될 수 있다

즉, 데이터 10개만 있으면 해당 공간을 나타낼 수 있다

[그림 10-2]에는 4개의 데이터가 공간 속에 표현되어 있음

<그림>

[그림 10-3]과 같은 2차원 공간을 생각해보자

이번에도 각 축은 0부터 10까지 표현할 수

그러나 차원이 하나 늘어났을 뿐인데, 공간을 가득 채우는 데 필요한 데이터는 100개로 1차원일 때 보다 10배가 늘어났음을 확인할 수 있다

<그림>

데이터 공간을 더 확장해 [그림 10-4]와 같은 3차원 공간을 생각해 보자

해당 공간을 채우는 데는 1,000개의 데이터가 필요하다

2차원 공간보다는 1차원 늘어났을 뿐인데, 900개의 데이터가 더 필요함

1차원 공간과 비교하면 990개의 데이터가 더 필요함

이처럼 차원이 하나씩 늘어날수록 해당 공간을 표현하는 데 필요한 데이터가 기하급수적으로 많아지는 현상을 차원의 저주(curse of dimensionality)라고 함

차원 축소 방법을 이용하면 이와 같은 차원의 저주 문제를 해결할 수 있음

이번 단원에서는 여러 가지 차원 축소 알고리즘에 대해 알아보자

# 10.2 주성분 분석

## 10.2.1 주성분 분석의 개념

**주성분 분석(Principal Component Analysis)**은 차원 축소 방법의 하나

주성분 분석은 여러 피처가 통계적으로 서로 상관관계가 없도록 변환시키는 방법

피처 p개가 있다고 가정하면 각 피처 벡터는 x_1, x_2, ... , x_p라고 나타낼 수 있음

주성분 분석은 오직 공분산 행렬(covariance matrix) \sigma에만 영향을 받음

피처 행렬 X를 그려 보면 아래와 같음

피처 행렬 X의 공분산 행렬을 \sum 라고 하고, 공분산 행렬의 대각 원소는 각 피처의 분산과 같으므로 \sigma^2_1, ...로 표기함

공분산 행열 \sum 의 고윳값을 \lambda_1 >= \lambda_2 >= ... >= \lambda_p, 고유 벡터를 e_1, e_2, ..., e_p 라고 함

각 고윳값과 고유 벡터는 쌍으로 (\lambda_1, e_1) , (), ... , ()로 표현할 수 있음

<그림>

[그림 10-5]를 보면 이해하기 쉬움

100 x 4 데이터 행렬 X가 존재할 때, 공분산을 계산하면 4 x 4 행렬 \sum 가 생성됨

해당 공분산 행렬의 고윳값을 구하면 4개의 고윳값 \lambda_1, ... 고유벡터 e_1, ...를 구할 수 있음

<br>

주성분 분석은 피처 간 상관관계를 기반으로 데이터의 특성을 파악

먼저 데이터 셋의 공분산 행렬의 고윳값과 고유 벡터를 구함

이때, 고윳값은 고유 벡터의 크기를 나타내며 분산의 크기를 의미

또한 고유 벡터는 분산의 방향을 의미

분산이 큰 고유 벡터에 기존 데이터를 투영해 새로운 데이터를 구할 수 있는데, 이렇게 구한 벡터를 주성분 벡터라고 부름

i번째 주성분 벡터를 y_i라고 하면 다음과 같이 수식으로 나타낼 수 있음

<그림>

새롭게 구한 i번째 주성분 벡터 y_i의 분산과 공분산을 구하면 아래와 같음

<그림>

위 식을 보면 주성분 벡터 간의 공분산은 0으로 서로 상관관계가 없다(uncorrelated)는 것을 알 수 있음

주성분 벡터는 서로 직교하는데, 직교한다는 뜻은 벡터 간 사잇각이 90도라는 뜻이고, 사잇각이 90도라는 뜻은 내적하면 0이라는 뜻

차원 축소를 하기 전 기존 데이터의 공분산 행렬을 구하면 피처 사이에 서로 상관관계가 존재할 확률이 높음

하지만 주성분 분석을 통해 차원 축소를 한 후의 주성분 벡터들은 서로 상관관계가 없다

왜냐하면 주성분 벡터가 서로 직교하며, 주성분 벡터가 직교한다는 것은 서로 상관관계가 없다는 것을 의미하기 때문

<그림>

[그림 10-6]은 앞선 식을 이용해 첫 번째 주성분 벡터 y_1을 계산한 결과

한편 지금까지 내용을 종합하면 아래와 같은 수식을 구할 수 있음

<그림>

위 수식에서 tr은 trace의 약자로 행렬의 주 대각선 성분의 합을 의미

그리고 \sum은 공분산 행렬이며, P는 고유벡터로 구성된 행렬, 즉 P = [e_1, e_2, ..., e_p]이다

그리고 A??은 대각 원소가 고윳값인 대각 행렬을 의미

주성분 분석 과정을 요약하면 다음과 같음

<그림>

위 순서를 잘 보면 3단계에서 공분산 행렬의 고윳값과 고유 벡터를 구하는데, 이는 어떻게 구할까요? 바로 선형대수 단원에서 배웠던 특이값 분해(singular value decomposition)를 사용함

즉, 공분산 행렬을 특이값 분해를 이용해 행렬을 분해하면 고윳값과 고유 벡터로 나눌 수 있음

<구분>

[그림 10-7]은 2차원 데이터의 주성분 벡터를 구한 예시

이때 구한 고윳값, 고유 벡터는 무슨 의미가 있을까?

행렬 분해의 대상이 되는 공분산 행렬을 통해 각 피쳐 분산을 알 수 있고, 서로 다른 피쳐 간 공분산을 알 수 있다

공분산을 안다는 뜻은 상관계수도 알 수 있다는 뜻

이는 분산을 알면 자연스럽게 표준 편차를 알 수 있는 것과 같음

이번에는 공분산 행렬을 통해 구한 고윳값과 고유 벡터의 의미를 살펴보자

우선 고유 벡터를 통해 알 수 있는 사실은 각 피처의 분산 방향을 알 수 있다

고윳값을 통해서는 분산의 크기를 알 수 있다

즉, 데이터가 여러 방향으로 흩어져 있을 때, 고유 벡터를 이용하면 각 흩어짐에 대한 방향을 파악할 수 있으며, 고윳값을 이용하면 어느 정도 흩어져 있는지 그 크기를 알 수 있다는 뜻

<그림>

주성분 벡터를 구한 후, 기존 데이터를 주성분 벡터로 투영시킬 수 있음

[그림 10-8]은 2차원 데이터를 1차원 주성분 벡터에 투영시킨 결과

이를 일반화시켜 공분산 행렬이 p차원 이라면 고윳값도 p개를 구할 수 있음

고윳값은 데이터의 흩어짐 정도의 크기라고 했으므로 고윳값 p개를 모두 더하면 데이터 셋의 전체 변동성이 됨

전체 변동성 대비 i번째 주성분이 설명하는 비율을 수식으로 나타내면 다음과 같음

<그림>

위 식을 이용하면 해당 고윳값이 전체 변동성의 크기 중 어느 정도를 설명하는지 알 수 있음

이를 설명된 분산이라고 함

예를 들어, p차원의 데이터를 2차원으로 줄이기로 한다면, 가장 큰 고윳값 두 개를 람다1, 람다2 라고 하자

이들의 설명된 분산을 구하기 위해 아래처럼 구했다고 하자

<그림>

위 결과를 통해 전체 변동성 중 람다1, 람다2로 설명되는 변동성이 94%라는 것을 알 수 있다

즉, 전체 p차원 데이터를 2차원으로 줄였을 때, 전체 변동성의 94%가 설명 가능하다는 뜻

## 10.2.2 주성분 분석 실습

# 10.3 커널 PCA

## 10.3.1 커널 PCA의 개념

기존 주성분 분석은 데이터 행렬 X의 공분산 행렬을 고윳값 분해한 후에 고유 벡터를 새로운 좌표축으로 할당하는 방법

기존의 데이터 포인트는 새로운 좌표축을 기준으로 새로운 좌표를 할당받음

이때 사용하는 새로운 좌표축에 해당하는 고유 벡터를 주성분(Principal Component)이라고 불렀으며, 기존 데이터 포인트를 주성분에 비교 정사영하는 방법을 사용했음

이에 반해 커널 PCA는 기존 PCA를 일반화한 방법으로 비선형적으로 수행하는 방법

<그림>

[그림 10-13]은 커널 PCA의 개념을 나타냄

우선 기존 데이터 행렬 X를 기존 데이터 행렬의 공간보다 큰 고차원 공간으로 매핑시킴

고차원 공간 매핑 함수를 (세타)라고 함

[그림 10-13]에서 1단계를 보면 기존 데이터 행렬은 2차원인데, (세타)를 어떻게 선택하냐에 따라 다른 공간으로 이동하게 됨

새로운 공간으로 이동한 데이터 포인트를 (세타)(X_i)라고 표현

새로운 공간으로 이동한 데이터에 대해 주성분 분석을 수행

즉, 새로운 공간의 데이터 포인트에 대해 아래와 같이 공분산 행렬을 구함

<그림>

위에서 구한 공분산 행렬을 기준으로 주성분 분석을 수행하면 [그림 10-13]에서 확인할 수 있듯, 새로운 공간에서 주성분 분석을 통해 새로운 좌표축이 생성된 것을 알 수 있음

마지막 단계로 3차원 공간의 데이터를 다시 원래 공간으로 매핑시키면 곡선 형태를 띤 주성분 축을 확인할 수 있음

## 10.3.2 커널 PCA 실습



# 10.4 LDA

## 10.4.1 LDA의 개념

LDA(Linear Discriminant Analysis)는 우리말로 선형 판별 분석

여기서 선형 판별이라는 말의 뜻은 데이터 포인트가 속하는 클래스를 구분하는 판별 함수가 선형(linear) 형태의 함수라는 뜻

LDA는 지도 학습적인 접근 방법을 통한 차원 축소 알고리즘

데이터 셋의 기존 공간으로부터 집단 간 분산과 집단 내 분산의 비율을 최대화하는 기존 공간보다 더 작은 차원의 공간으로 원래 데이터를 토영시킴으로써 차원 축소

즉, 데이터를 최대한 분리해 주는 기능을 함

<그림>

LDA는 크게 3단계 과정을 거침

첫 번째 단계로, 집단 간 거리를 계산

이를 집단 간 분산(between-class variance)이라고 함

두 번째 단계는 각 집단 간의 평균과 각 데이터 포인트의 거리를 계산함

이를 집단 내 분산(within-class variance)라고 함

마지막 세 번째 단계는 기존 데이터 셋의 공간보다 더 작은 차원의 공간을 만드는 것

이 공간은 집단 간 분산을 최대화하고 집단 내 분산을 최소로 하는 공간이여야 함

<그림>

예를 들어, 전체 데이터 셋 행렬을 X = {x_1, x_2, ..., x_n} 이라고 함

이 때 x_i는 i번째 데이터 포인트, n은 데이터 셋 전체 데이터 개수

각 데이터 포인트는 p개의 피처로 구성되어 있다고 가정

데이터 셋이 2개의 집단(c = 2)으로 구분되어 있다고 가정

즉, 전체 데이터 셋 X는 X = {X_1, X_2}로 서로 다른 2개의 집단 데이터로 생각할 수 있음

각 집단은 4개의 데이터 포인트로 구성되어 있다고 가정 (n1 = n2 = 4)

전체 데이터셋을 구성하는 데이터 개수는 8개가 됨(n = 8)

### 집단 간 분산 구하기

집단 간 분산을 계산해보자

i번째 집단의 집단 간 분산이 의미하는 것은 데이터 셋 전체 평균 \마이크로 와 i번째 집단에 속하는 데이터의 평균 \마이크로_i 간의 차이

LDA는 기존 데이터 셋 공간보다 더 작은 차원의 공간들 중에 집단 간 거리를 최대화하는 공간을 찾는 것이 목적

<그림>

[그림 10-19] 는 집단 내 분산을 구하는 과정을 나타내는 그림

이렇게 구한 집단 내 공분산 행렬을 \sum_w 라고 함

이때, W는 '집단 내 분산(within-class variance)'이라는 이름에서 within의 약자를 나타냄

### 더 작은 차원의 공간 생성

집단 간 공분산 행렬 \sum_B와 집단 내 공분산 행렬 \sum_W를 게산함

그리고 이를 이용해 \sum_W^-1 \sum_B 행렬의 고윳값과 고유 벡터를 구한 후, 고윳값 집합을 \lambda = {\lambda_1, \lambda_2, ... ,\lambda_p} 와 고유 벡터의 집합 행렬을 V = {v_1, v_2, ..., v_p}라고 함

<그림>

여기서 각각의 고윳값은 스칼라값이고, 각각의 고유 벡터는 벡터

이들을 LDA가 만드는 새로운 공간에 대한 정보를 줌

고윳값을 큰 순서대로 정렬했을 때, 각 고윳값에 대응되는 고유 벡터가 새로운 공간의 축이 됨

예를 들어, 가장 큰 고윳값에 대응되는 고유 벡터가 새로운 공간의 첫번째 축이 되는 것

각 고유 벡터는 LDA가 만드는 쌔로운 공간의 축을 나타내므로, k개의 가장 큰 고윳값에 대응하는 고유 벡터는 더 작은 차원의 공간 V_k를 만듬

<그림>

줄어든 공간은 k 차원이므로 p-k 개의 피처는 무시됨

각 데이터 샘플을 p차원에서 k차원 공간으로 투영하여 x^' = x^TV_k 로 나타내고, 데이터 행렬 전체로 표현하면 아래와 같이 나타낼 수 있음

<그림>

[그림 10-22]는 더 작은 공간 2개를 비교한 것

<그림>

[그림 10-22]에서 데이터는 2개의 집단으로 구성되어 있음

각 집단은 4개의 데이터 포인트로 구성되어 있고, 각 데이터는 2개의 피처로 구성되어 있음

변환 행렬 W는 2x2 행렬이고, W의 고윳값 \lambda_1,\lambda_2, 고유 벡터 v_1,v_2를 계산함

여기서 2개의 고유 벡터라고 볼 수도 있고, 두 개의 부분 공간이라고 볼 수도 있음

2개의 부분 공간을 비교하면 아래와 같은 사실을 알 수 있음

<그림>

첫째, 첫 번째 고유 벡터 v_1에 투영했을 때의 집단 간 분산은 두 번째 벡터 v_2에 투영했을 떄보다 크다는 것을 알 수 있음

이 그림에서 두 개의 집단은 v_1에 투영했을 때 효과적으로 구분되는 것을 보여줌

그리고 새로운 데이터에 대한 예측은 변형된 공간ㄴ에서의 각 집단별 평균을 구해 가장 가까운 집단으로 구분함

둘째, v_1에 투영했을 때의 집단 내 분산은 v_2에 투영했을 때의 집단 내 분산보다 작음

v_1에 투영했을 때의 집단 내 분산이 더 작으므로, v_1에 투영하는 것이 더 효과적

위 두 내용을 종합하면 LDA의 목표를 달성하기 위해 첫 번째 고유 벡터 v_1이 두 번째 고유 벡터 v_2보다 더 적절한 부분 공간이라고 결론 지을 수 있음


LDA에는 두 가지 방법이 존재

하나는 Class-Dependent LDA이고, 다른 하나는 Class-Independent LDA

Class-Dependent LDA에서는 하나의 부분 공간은 각 집단으로 계산됨

즉, i번째 집단의 변환 행렬은 W_i = \sum_W^-1\sum_B 로 나타냄

고윳값 고유 벡터 또한 각 변환 행렬로 따로 구함

각 집단 데이터 포인트는 그들에 대응하는 고유 벡터에 투영됨

이와는 다르게 Class-Independent LDA에서는 클래스 구분 없이 모든 클래스에 대해 변환 행렬을 계산

<그림>
<그림>
<그림>
<그림>

LDA의 기본 개념은 PCA와 비슷

PCA는 데이터 셋의 분산이 최대인 직교 성분 축을 찾으려고 하는 반면, LDA는 클래스를 최적으로 구분할 수 있는 특성 부분 공간을 찾는 것

또한 PCA는 비지도 학습(unsupervised learning)인 반면에 LDA는 지도 학습임

## 10.4.2 LDA의 이론적 배경

이번 단원에서는 LDA를 더욱 이론적으로 살펴봄

LDA를 통해서 하려는 목적은 데이터가 주어질 때, 해당 데이터의 집단을 구분하는 것

이를 수식으로 나타내면 다음과 같음

P(C|X)

P(C|X)란 클래스 사후 확률(posterior probability)을 의미하는데, 이는 주어진 데이터 X가 집단 C에 속할 확률을 의미

그리고 데이터 포인트별로 각 집단에 속할 확률을 구해서 가장 높은 확률을 보이는 집단에 속할 것으로 예측하는 것

위 확률을 구하기 위해 베이즈 정리를 사용하면 아래와 같이 표현할 수 있음

<그림>

위 식에서 f_k(x)는 클래스 k에 속하는 확률 변수 X의 확률 밀도 함수를 나타내고, \pi_k 는 집단 k의 사전 확률을 의미

이는 전체 데이터셋 샘플 개수 대비 클래스 k에 속하는 데이터셋의 비율을 의미

<그림>

이를 정리해 그림으로 나타내면 [그림 10-24]와 같음

<그림>

집단별 확률 밀도 함수 f_k(x)가 다변량 정규 분포(Multivariate normal distribution)를 다른다고 가정하면, f_k(x)의 확률 밀도 함수는 아래와 같이 나타낼 수 있음

<그림>

위 식에서 집단 k에 속하는 데이터의 공분산 행렬을 \sum_k 라고 했는데, LDA에서는 모든 집단별 확률 밀도 함수가 모두 같은 공분산을 가지고 있다고 가정

아래와 같이 표현할 수 있음

<그림>

새로운 데이터가 집단 k에 속하는지 아니면 다른 집단 l에 속하는지 알기 위해서 다음과 같은 두 클래스 사후 확률의 로그 비율을 이용

만약 주어진 데이터가 클래스 k에 속할 확률이 높다면 다음 로그 비율은 양수 값을 가질 것이고, 만약 주어진 데이터가 클래스 l에 속할 확률이 크다면 다음 로그 비율은 음수 값을 가질 것

<그림>

위 식은 앞서 구한 클래스 사후 확률과 확률 밀도 함수를 대입한 후 전개한 것

위 식을 정리하면 아래와 같이 나타낼 수 있음

<그림>

위 식은 x에 대한 선형방정식

선형 판별 분석이라는 이름에서 '선형'이라는 단어가 어디서 나왔는지 알 수 있음

위 식의 의미는 집단 k와 집단 l의 경계를 결정짓는 집단 간 결정 경계(decision boundary)가 P(G = k|X = x) = P(G = l | X = x)를 만족하는 초평면(hyperplane)을 의미한다는 뜻

왜냐하면 P(G = k|X = x) = P(G = l|X = x)라는 말은 주어진 데이터가 집단 k에 속할 확률과 집단 l에 속할 확률이 같다는 뜻이므로, 이 지점이 두 집단을 구분하는 경계가 됨

<그림>

앞선 수식을 참고해, 클래스 k에 대한 선형 판별 함수(linear discriminant function)를 \delta_k(x)라고 하면 \delta_k(x)는 아래와 같음

<그림>

위 식을 이용해 새로운 데이터 x가 어떤 집단에 속하는지는 C(x) = argmax\delta_k(x)를 구하면 알 수 있음

즉, 가장 큰 \delta_k(x)값을 나타내는 집단 k에 속한다고 볼 수 있음

\delta_k(x)값이 크다는 말은 해당 데이터 x가 집단 k에 속할 확률이 높다는 뜻

이번에는 실제로 데이터가 주어질 때를 생각해보자

위에서 우리는 데이터가 다변량 정규 분포를 따른다고 가정하고 시작했음

이래와 같이 다변량 정규 분표의 파라미터(parameter)를 추정하면서 분석을 시작

<그림>

위 식에서 n은 전체 데이터 개수를 의미, n_k는 집단 k에 속하는 데이터 개수 의미

예를 들어, 두 가지 클래스 1,2로 구성된 데이터가 있다고 가정할 때 어떤 데이터가 클래스 2에 속하려면 아래와 같은 조건을 만족해야 함

<그림>

위 식을 조금 바꾸면 아래와 같이 쓸 수 있음

<그림>

LDA에서 판별 함수는 선형이지만 데이터의 집단을 판별하는 선이 항상 직선인 것은 아님

LDA를 이용하여 곡선의 형태로 판별식을 그릴 수 있음

그 방법은 바로 데이터를 기존 공간보다 더 높은 차원의 공간으로 선형 변환한 후 확장된 공간에서 LDA를 하는 것

<그림>

예를 들어, [그림 10-26]와 같이 데이터가 2차원 공간에 있다고 하면 데이터의 좌표를 x = (x_1,x_2)로 나타낼 수 있음

이러한 2차원 데이터를 3차원으로 확장해보자

x^* = (x_1,x_2,x_1x_2)로 확장한 후 확장된 공간에서 LDA를 사용해서 판별하면 결정 경계는 곡선 형태를 띔

앞선 LDA에서는 클래스 집단별 공분산 행렬 \sum_k가 모두 동일하다고 가정

QDA(Quadratic DIscriminant Analysis)는 이 가정을 일반화시킨 방법으로 집단별 공분산 행렬이 같다는 가정이 존재하지 않음

집단별 로그 비율에서 약분되는 부분이 없어지고 이차 판별 함수(quadriatic discriminant function)는 아래와 같게 됨

<그림>

위 판별식에서 볼 수 있듯, 클래스 k 집단과 클래스 l 집단의 결정 경계는 \delta_k(x) = \delta_l(x)인 초평면이고 이차식을 따름

LDA에서는 판별 함수가 선형이였던 것과 반대로 QDA에서는 이차식을 따르는 것을 확인할 수 있음

LDA, QDA는 파라미터 추정치의 분산이 작다는 장점이 존재하지만 bias가 존재한다는 단점이 있음

LDA와 QDA 계산의 핵심은 공분산 행렬을 고윳값 분해를 하는 것

QDA의 경우 ???에 대해 ????로 고윳값 분해를 함

이 때, U_k는 p x p 직교 행렬이며, D_k는 양수인 고윳값 \lambda_kl의 대각 행렬

이렇게 되면 앞서 언급한 판별 함수 \delta_k(x)의 구성 요소는 아래와 같이 바뀌게 됨

아래 식에서는 직교 행렬의 역행렬은 기존 행렬의 전치 행렬이라는 성질을 이용

<그림>

위 식을 바탕으로 LDA를 생각해 보면 다음과 같음

공통 분산 ???를 고윳값 분해하고 기존 데이터 행렬 X를 X^' = D^{(-1/2)}U^TX 로 변형시키면, 선형 변환 데이터 X^'의 공분산 행렬은 단위 행렬(identity matrix)이 됨

이와 같이 공분산 행렬이 단위 행렬이 되도록 기존 데이터를 변형하는 방법을 Whitening Transformation이라고 함

단위 행렬의 대각 원소는 1이므로 이는 각 피처의 분산이 1이라는 의미

그리고 단위 행렬의 대각 원소를 제외한 나머지 원소는 0이므로 이는 변수 간 상관관계가 존재하지 않는 것을 의미

따라서 각 데이터 포인트는 변형된 공간에서 각 집단 별 평균과의 차이를 구해 가장 가까운 집단으로 판별하게 됨

<그림>

지금까지 다룬 LDA 알고리즘을 사용하기 위해서는 데이터가 다변량 정규 분포를 따른다는 가정이 필요함

하지만 초기 LDA는 시간이 지나면서 다변량 정규 분포를 가정하지 않는 방법으로 발전함

LDA는 집단 간 분산 \sum_B 는 최대화하고, 집단 내 분산 \sum_W는 최소화하는 선형 조합(linear combination) X^' = Xa를 찾는 문제

기존 데이터 행렬 X를 새로운 공간에 투영해서 새로운 데이터 X'가 됨

이 때, '집단 간 분산' 이라는 말의 의미는 X'행렬의 각 클래스 집단별 평균의 차이를 의미

또한 '집단 내 분산'은 각 집단별 평균에 대한 해당 집단의 데이터 합동 분산(pooled variance)를 의미

선형 변환된 데이터 X'의 집단 간 분산은 a^T\sum_Ba 와 같고 변환된 데이터 X'의 집단 내 분산은 a^T\sum_Wa

LDA를 위해 집단 간 분산을 최대회하고, 집단 내 분산을 최소화한다는 말을 수식으로 나타내면 아래와 같음

<그림>

위 식에서 우리는 벡터의 길이에 관심이 있는 것이 아니라, 벡터의 방향에만 관심이 있음

따라서 벡터의 길이를 고정시켜서 위 식을 다시 쓰면 아래와 같음

<그림>

즉, 선형 판별 분석은 a^T\sum_Wa = 1 이라는 제약 조건 하에 a^T\sum_Ba 을 최대화하는 벡터 a를 찾는 것과 같음

위 최적화 식을 라그랑지안 형태로 표현할 수 있음

<그림>

앞선 최적화 식에서는 \lambda는 라그랑지안 승수

라그랑지안 식을 a로 미분해 최적값을 구해보자

<구분>

위 식에서 \lambda는 >??????????의 ㅁㄹㄴㅁㄹㅁ



## 10.4.3 LDA 실습


# 10.5 LLE

## 10.5.1 LLE의 개념

여러 가지 차원 축소 방법을 이용해 데이터 차원을 변형시키는데, 만약 차원 축소 전 데이터의 특징을 잘 잡아낸다면 차원 숙소 이후에 데이터를 시각화했을 때도 데이터가 잘 구분될 것

LLE(Locally Linear Embedding)는 각 데이터 포인트의 이웃 데이터 중 K개 이웃 데이터를 선정하고 데이터 자신과 K개의 이웃 데이터를 마치 하니의 덩어리라고 가정하고 데이터를 재구성

LLE는 기존 데이터를 자신의 이웃 데이터를 기준으로 재구성하는 방법

즉, 해당 데이터 자체보다 주변의 이웃 데이터들이 중요

<그림>

[그림 10-29]는 3차원 데이터를 가정하고 4개의 이웃을 선택한 것을 나타냄

즉, x_i 데이터 포인트를 기준으로 4개의 이웃을 선택했으며 x_j는 4개의 이웃 중 하나가 됨

데이터를 재구성할 때는 [그림 10-29]에서 선택한 이웃을 기준으로 재구성

이 때, 재구성 에러를 최소화해야 하는데, 재구성 에러는 아래와 같음

<그림>

위 식에서 x_i는 각 데이터 포인트를 벡터 형태로 표시한 것

w_ij는 가중치를 의미하는데, 이는 i번째 데이터 포인터를 재구성할 때 j번째 데이터의 기여도를 나타냄

이를 그림으로 나타내면 [그림 10-30]과 같음

<그림>

LLE의 목적은 위에서 언급한 재구성 에러 함수를 최소화하는 것이므로 재구성 에러 함수가 목적 함수에 해당한다고 볼 수 있음

위 목적 함수에는 두 가지 제약 조건이 존재

우선 각 데이터 포인트는 오직 이웃 데이터로만 재구성

처음에 K개의 이웃 데이터를 정하게 되는데, 만약 x_j가 이웃 데이터에 속하지 않는다면 w_ij = 0

그리고 전체 가중치 행렬 W의 각 행 원소(row element)의 합은 1

즉, \sum_jw_ij = 1

<그림>

데이터를 차원 축소

[그림 10-31]은 3차원 데이터를 2차원 데이터로 차원 축소한 것을 나타냄

기존 공간에 존재하는 데이터 x_i는 새로운 공간의 데이터 y_i가 됨

LLE에서는 데이터 주변의 이웃 관계를 유지하면서 새로운 공간에 투영하는 것이 중요

<그림>

차원 축소 데이터 y_i는 위와 같은 임베딩 비용 함수를 최소화하는 값으로 구할 수 있음

즉, 이 문제는 새로운 공간에서의 데이터 y_i와 y_i 주변에 존재하는 데이터 y_j 간의 최소화하는 문제와 동일

## 10.5.2 LLE 실습



# 10.6 비음수 행렬 분해

## 10.6.1 비음수 행렬 분해의 개념

비음수 행렬 분해(Non-negative Matrix Factorization, NMF)는 데이터 행렬 내 모든 원소값이 0보다 큰 경우에 사용할 수 있는 행렬 분해 방법

<그림>

비음수 행렬 분해는 데이터 행렬 X가 주어질 때 다음 조건을 만족하는 비음수 행렬(non-negative matrix) U, V를 찾는 알고리즘

<그림>

앞선 식에서 행렬 X의 차원을 n x p라고 하면 행렬 U는 n x d이며 행렬 V의 차원은 d x p 이다

이때, d는 n, p보다 작은 숫자를 선택

즉, 앞선 식의 의미는 행렬 X를 차원이 작은 두 행렬 U, V로 분해한다는 의미

그리고 U, V의 원소가 비음수(non-negative)이므로 비음수 행렬 분해(non-negative matrix factorization)이라는 이름이 붙은 것

비음수 행렬 분해는 앞선 공식처럼 행렬 형태로 표현할 수도 있지만, 벡터 형태로 표현할 수도 있음

비음수 행렬 분해를 벡터 형태로 표현하면 아래와 같이 쓸 수 있음

<그림>

위 식에서 x_i는 데이터 행렬 X의 벡터에 해당하며, v_i는 V의 벡터

즉, 데이터 행렬 X의 벡터 x_i는 행렬 U의 각 열에 가중치 v_i를 적용한 선형 결합 형태라고 생각할 수 있음

행렬 U는 데이터 행렬 X의 근사 기저라고 생각할 수 있음

앞선 비음수 행렬 분해 조건에서 행렬 U의 차원은 데이터 행렬 X보다 작았음

이는 데이터 행렬 X를 표현하는 데 상대적으로 크기가 작은 수의 기저 벡터로 표현할 수 있다는 의미

동일한 크기의 데이터 행렬 X를 표현하는 데 더 작은 수의 기저 벡터를 사용할 수록 성능이 뛰어나다고 할 수 있음

## 10.6.2 비용 함수

비음수 행렬 분해는 어떻게 할 수 있을까

이를 위해 먼저 비용 함수에 대해서 알아보자

비음수 행렬 분해에서 사용하는 비용 함수는 여러 가지가 있지만 가장 기본적인 비용 함수인 유클리디안 거리를 사용

<그림>

유클리디안 거리는 작을수록 높은 성능을 의미하며 0이 되면 최고의 성능을 보여줌

비음수 행렬 분해는 비용 함수를 최소화하는 행렬 U, V를 찾는 것

비음수 행렬 분해 알고리즘 과정을 정리함녀 다음과 같음

<그림>

## 10.6.3 비음수 행렬 분해 실습