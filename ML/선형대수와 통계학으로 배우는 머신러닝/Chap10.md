차원 축소

# 10.1 차원 축소 개념

## 10.1.1 차원 축소하는 이유

데이터에는 중요한 부분과 중요하지 않은 부분이 있음

여기서 중요하지 않은 부분을 노이즈(noise)라고 함

노이즈는 데이터에서 정보를 얻을 때 방해가 되는 부분

머신러닝 과정에서는 이러한 불필요한 노이즈를 제거하는 것이 중요

그리고 노이즈를 제거할 때 사용하는 방법이 차원 축소(dimension reduction)

차원 축소를 통해 데이터의 중요하지 않은 부분인 노이즈를 제거할 수 있기에 널리 사용되는중

<그림>

[그림 10-1]의 고양이는 3차원 공간에 있음

우리는 고양이의 모습을 보고 고양이라고 인식함

그러나 [그림 10-1]에서 고양이를 직접 보지 않고 고양이라고 인식할 방법이 하나 더 있음

그것은 고양이의 그림자를 보는 경우

즉, 3차원 공간의 고양이가 아니라 2차원 평면의 그림자만 보고도 고양이라고 인식하는 것

이는 고양이를 인식하는 데 3차원 데이터가 아닌 2차원 데이터만으로도 충분하다는 뜻

한편 고양이를 그림자만 보고 판단하려면 햇빛의 각도가 중요

그림자는 햇빛의 각도에 따라 변하는데, 너무 짧거나 길면 고양이라고 인식할 확률이 낮아짐

이처럼 차원 축소는 주어진 데이터의 정보 손실을 최소화하면서 줄이는 것이 핵심

차원 축소는 특징을 추출한다는 것과 밀접한 관련이 있음

특징을 추출한다는 것은 데이터에서 두드러진 면을 찾는 것과 같음

앞서 고양이의 예에서 그림자는 고양이의 중요한 특징을 포함해야 함

즉, 그림자는 고양이라고 판단할 수 있을 법한 정보를 포함해야 한다는 뜻

<br>

차원 축소는 비지도 학습, 지도 학습과 마찬가지로 비지도 학습 차원 축소로 접근할 수도 있고, 지도 학습 차원 축소로 접근할 수도 있음

비지도 학습적인 접근 방법에는 대표적으로 주성분 분석(Principal Component Analysis)과 같은 방법이 있고, 지도 학습적인 접근 방법에는 선형 판별 분석(Linear Discriminant Analysis)과 같은 방법이 있음

## 10.1.3 차원의 저주

차원 축소를 하는 이유 중 하나는 차원의 저주 문제를 해결할 수 있기 때문

**차원의 저주(curse of dimensionality)**란 데이터의 차원이 커질수록 해당 차원을 표현하기 위해 필요한 데이터가 기하급수적으로 많아짐을 의미

예를 들어, 1차원 직선 공간을 생각해 보자

만약 1차원 직선이 2차원 평면만 되어도 단지 1차원 늘어났을 뿐인데, 해당 공간을 나타내기 위해 필요한 데이터는 크게 늘어남

이처럼 차원이 하나씩 늘어날수록 필요한 데이터는 기하급수적으로 많아짐

즉, 트레이닝 데이터 셋의 차원이 클수록 차원의 저주 때문에 해당 공간을 적절히 표현하지 못하여 오버피팅될 확률이 높아짐

다음과 같이 예를 들어보자

<그림>

[그림 10-2]와 같이 1차원 공간에 데이터를 표현할 수 있는 공간이 0부터 10까지 있다고 하면 해당 공간에는 10개의 데이터가 포함될 수 있다

즉, 데이터 10개만 있으면 해당 공간을 나타낼 수 있다

[그림 10-2]에는 4개의 데이터가 공간 속에 표현되어 있음

<그림>

[그림 10-3]과 같은 2차원 공간을 생각해보자

이번에도 각 축은 0부터 10까지 표현할 수

그러나 차원이 하나 늘어났을 뿐인데, 공간을 가득 채우는 데 필요한 데이터는 100개로 1차원일 때 보다 10배가 늘어났음을 확인할 수 있다

<그림>

데이터 공간을 더 확장해 [그림 10-4]와 같은 3차원 공간을 생각해 보자

해당 공간을 채우는 데는 1,000개의 데이터가 필요하다

2차원 공간보다는 1차원 늘어났을 뿐인데, 900개의 데이터가 더 필요함

1차원 공간과 비교하면 990개의 데이터가 더 필요함

이처럼 차원이 하나씩 늘어날수록 해당 공간을 표현하는 데 필요한 데이터가 기하급수적으로 많아지는 현상을 차원의 저주(curse of dimensionality)라고 함

차원 축소 방법을 이용하면 이와 같은 차원의 저주 문제를 해결할 수 있음

이번 단원에서는 여러 가지 차원 축소 알고리즘에 대해 알아보자

# 10.2 주성분 분석

## 10.2.1 주성분 분석의 개념

**주성분 분석(Principal Component Analysis)**은 차원 축소 방법의 하나

주성분 분석은 여러 피처가 통계적으로 서로 상관관계가 없도록 변환시키는 방법

피처 p개가 있다고 가정하면 각 피처 벡터는 x_1, x_2, ... , x_p라고 나타낼 수 있음

주성분 분석은 오직 공분산 행렬(covariance matrix) \sigma에만 영향을 받음

피처 행렬 X를 그려 보면 아래와 같음

피처 행렬 X의 공분산 행렬을 \sum 라고 하고, 공분산 행렬의 대각 원소는 각 피처의 분산과 같으므로 \sigma^2_1, ...로 표기함

공분산 행열 \sum 의 고윳값을 \lambda_1 >= \lambda_2 >= ... >= \lambda_p, 고유 벡터를 e_1, e_2, ..., e_p 라고 함

각 고윳값과 고유 벡터는 쌍으로 (\lambda_1, e_1) , (), ... , ()로 표현할 수 있음

<그림>

[그림 10-5]를 보면 이해하기 쉬움

100 x 4 데이터 행렬 X가 존재할 때, 공분산을 계산하면 4 x 4 행렬 \sum 가 생성됨

해당 공분산 행렬의 고윳값을 구하면 4개의 고윳값 \lambda_1, ... 고유벡터 e_1, ...를 구할 수 있음

<br>

주성분 분석은 피처 간 상관관계를 기반으로 데이터의 특성을 파악

먼저 데이터 셋의 공분산 행렬의 고윳값과 고유 벡터를 구함

이때, 고윳값은 고유 벡터의 크기를 나타내며 분산의 크기를 의미

또한 고유 벡터는 분산의 방향을 의미

분산이 큰 고유 벡터에 기존 데이터를 투영해 새로운 데이터를 구할 수 있는데, 이렇게 구한 벡터를 주성분 벡터라고 부름

i번째 주성분 벡터를 y_i라고 하면 다음과 같이 수식으로 나타낼 수 있음

<그림>

새롭게 구한 i번째 주성분 벡터 y_i의 분산과 공분산을 구하면 아래와 같음

<그림>

위 식을 보면 주성분 벡터 간의 공분산은 0으로 서로 상관관계가 없다(uncorrelated)는 것을 알 수 있음

주성분 벡터는 서로 직교하는데, 직교한다는 뜻은 벡터 간 사잇각이 90도라는 뜻이고, 사잇각이 90도라는 뜻은 내적하면 0이라는 뜻

차원 축소를 하기 전 기존 데이터의 공분산 행렬을 구하면 피처 사이에 서로 상관관계가 존재할 확률이 높음

하지만 주성분 분석을 통해 차원 축소를 한 후의 주성분 벡터들은 서로 상관관계가 없다

왜냐하면 주성분 벡터가 서로 직교하며, 주성분 벡터가 직교한다는 것은 서로 상관관계가 없다는 것을 의미하기 때문

<그림>

[그림 10-6]은 앞선 식을 이용해 첫 번째 주성분 벡터 y_1을 계산한 결과

한편 지금까지 내용을 종합하면 아래와 같은 수식을 구할 수 있음

<그림>

위 수식에서 tr은 trace의 약자로 행렬의 주 대각선 성분의 합을 의미

그리고 \sum은 공분산 행렬이며, P는 고유벡터로 구성된 행렬, 즉 P = [e_1, e_2, ..., e_p]이다

그리고 A??은 대각 원소가 고윳값인 대각 행렬을 의미

주성분 분석 과정을 요약하면 다음과 같음

<그림>

위 순서를 잘 보면 3단계에서 공분산 행렬의 고윳값과 고유 벡터를 구하는데, 이는 어떻게 구할까요? 바로 선형대수 단원에서 배웠던 특이값 분해(singular value decomposition)를 사용함

즉, 공분산 행렬을 특이값 분해를 이용해 행렬을 분해하면 고윳값과 고유 벡터로 나눌 수 있음

<구분>

[그림 10-7]은 2차원 데이터의 주성분 벡터를 구한 예시

이때 구한 고윳값, 고유 벡터는 무슨 의미가 있을까?

행렬 분해의 대상이 되는 공분산 행렬을 통해 각 피쳐 분산을 알 수 있고, 서로 다른 피쳐 간 공분산을 알 수 있다

공분산을 안다는 뜻은 상관계수도 알 수 있다는 뜻

이는 분산을 알면 자연스럽게 표준 편차를 알 수 있는 것과 같음

이번에는 공분산 행렬을 통해 구한 고윳값과 고유 벡터의 의미를 살펴보자

우선 고유 벡터를 통해 알 수 있는 사실은 각 피처의 분산 방향을 알 수 있다

고윳값을 통해서는 분산의 크기를 알 수 있다

즉, 데이터가 여러 방향으로 흩어져 있을 때, 고유 벡터를 이용하면 각 흩어짐에 대한 방향을 파악할 수 있으며, 고윳값을 이용하면 어느 정도 흩어져 있는지 그 크기를 알 수 있다는 뜻

<그림>

주성분 벡터를 구한 후, 기존 데이터를 주성분 벡터로 투영시킬 수 있음

[그림 10-8]은 2차원 데이터를 1차원 주성분 벡터에 투영시킨 결과

이를 일반화시켜 공분산 행렬이 p차원 이라면 고윳값도 p개를 구할 수 있음

고윳값은 데이터의 흩어짐 정도의 크기라고 했으므로 고윳값 p개를 모두 더하면 데이터 셋의 전체 변동성이 됨

전체 변동성 대비 i번째 주성분이 설명하는 비율을 수식으로 나타내면 다음과 같음

<그림>

위 식을 이용하면 해당 고윳값이 전체 변동성의 크기 중 어느 정도를 설명하는지 알 수 있음

이를 설명된 분산이라고 함

예를 들어, p차원의 데이터를 2차원으로 줄이기로 한다면, 가장 큰 고윳값 두 개를 람다1, 람다2 라고 하자

이들의 설명된 분산을 구하기 위해 아래처럼 구했다고 하자

<그림>

위 결과를 통해 전체 변동성 중 람다1, 람다2로 설명되는 변동성이 94%라는 것을 알 수 있다

즉, 전체 p차원 데이터를 2차원으로 줄였을 때, 전체 변동성의 94%가 설명 가능하다는 뜻

## 10.2.2 주성분 분석 실습

# 10.3 커널 PCA

## 10.3.1 커널 PCA의 개념

기존 주성분 분석은 데이터 행렬 X의 공분산 행렬을 고윳값 분해한 후에 고유 벡터를 새로운 좌표축으로 할당하는 방법

기존의 데이터 포인트는 새로운 좌표축을 기준으로 새로운 좌표를 할당받음

이때 사용하는 새로운 좌표축에 해당하는 고유 벡터를 주성분(Principal Component)이라고 불렀으며, 기존 데이터 포인트를 주성분에 비교 정사영하는 방법을 사용했음

이에 반해 커널 PCA는 기존 PCA를 일반화한 방법으로 비선형적으로 수행하는 방법

<그림>

[그림 10-13]은 커널 PCA의 개념을 나타냄

우선 기존 데이터 행렬 X를 기존 데이터 행렬의 공간보다 큰 고차원 공간으로 매핑시킴

고차원 공간 매핑 함수를 (세타)라고 함

[그림 10-13]에서 1단계를 보면 기존 데이터 행렬은 2차원인데, (세타)를 어떻게 선택하냐에 따라 다른 공간으로 이동하게 됨

새로운 공간으로 이동한 데이터 포인트를 (세타)(X_i)라고 표현

새로운 공간으로 이동한 데이터에 대해 주성분 분석을 수행

즉, 새로운 공간의 데이터 포인트에 대해 아래와 같이 공분산 행렬을 구함

<그림>

위에서 구한 공분산 행렬을 기준으로 주성분 분석을 수행하면 [그림 10-13]에서 확인할 수 있듯, 새로운 공간에서 주성분 분석을 통해 새로운 좌표축이 생성된 것을 알 수 있음

마지막 단계로 3차원 공간의 데이터를 다시 원래 공간으로 매핑시키면 곡선 형태를 띤 주성분 축을 확인할 수 있음

## 10.3.2 커널 PCA 실습



# 10.4 LDA

## 10.4.1 LDA의 개념

## 10.4.2 LDA의 이론적 배경

## 10.4.3 LDA 실습

# 10.5 LLE

## 10.5.1 LLE의 개념

## 10.5.2 LLE 실습



# 10.6 비음수 행렬 분해

## 10.6.1 비음수 행렬 분해의 개념

## 10.6.2 비용 함수

## 10.6.3 비음수 행렬 분해 실습